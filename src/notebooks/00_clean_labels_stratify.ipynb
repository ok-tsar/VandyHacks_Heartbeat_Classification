{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../data/train: File exists\n",
      "mkdir: ../data/valid: File exists\n",
      "mkdir: ../data/test: File exists\n"
     ]
    }
   ],
   "source": [
    "# Make directories for each one of our data sets\n",
    "%mkdir ../data/train\n",
    "%mkdir ../data/valid\n",
    "%mkdir ../data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set1 = pd.read_csv(Path(DIR_DATA, 'set_a.csv'))\n",
    "df_set2 = pd.read_csv(Path(DIR_DATA, 'set_b.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! they have the same structure! to make things easy, we'll combine them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 5)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files = pd.concat([df_set1, df_set2], axis = 0).reset_index()\n",
    "\n",
    "df_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         0\n",
       "dataset       0\n",
       "fname         0\n",
       "label       247\n",
       "sublabel    683\n",
       "dtype: int64"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many unknown sublabels - going to drop them\n",
    "df_files = df_files.drop('sublabel', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets look at the labels! To use `PyTorch` to train a CNN as a multiclass classifier, we need to structure our files so we can easily tell which files have which label, without relying on a CSV that could easily be corrupted or edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining extrastole and exrahls since they're distinct in both set a and set b.\n",
    "df_test['label2'] = df_test['label']\n",
    "df_test.loc[(df_test['label2'] == 'extrahls') | (df_test['label2'] == 'extrastole'), 'label2'] = 'extra'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our own labels\n",
    "After much trial and error, turns out that the actual file names actually match the file names in the csvs. This makes it incredibly difficult to properly segment and organize them. Fortunately each filename has its classification at the begginging, we are just going to grab it directly\n",
    "\n",
    "The unlabeled data will be left behind since we don't have it classified anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "dataset = []\n",
    "\n",
    "for i in ['set_a', 'set_b']:\n",
    "    f = os.listdir(Path(DIR_DATA, i))\n",
    "    file_names.extend(f)\n",
    "    dataset.extend([i] * len(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification = pd.DataFrame({\n",
    "    'dataset': dataset,\n",
    "    'file_name': file_names\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unlabelled\n",
    "df_classification = df_classification.loc[~df_classification['file_name'].str.contains('unlabelled')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label from filename\n",
    "df_classification.loc[:, 'label'] = df_classification['file_name'].str.split('_').str[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'extrahls' and 'extrastole' to 'extra'\n",
    "df_classification.loc[df_classification['label'].str.contains('extra'), ['label']] = 'extra'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratification\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_classification['label']\n",
    "y = df_classification[['dataset', 'file_name', 'label']] # OK to have label - Not using in SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split of our files\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.20,\n",
    "                                                    stratify = y[['dataset', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subfolder for each label\n",
    "for i in y['label'].unique():\n",
    "    try:\n",
    "        os.makedirs(Path(DIR_DATA,'train', i)) # Training\n",
    "        os.makedirs(Path(DIR_DATA,'valid', i)) # Validation\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Files\n",
    "for index, val in y_train.iterrows():\n",
    "    \n",
    "    orig_path = Path(DIR_DATA, val['dataset'], val['file_name'])\n",
    "    dest_path = Path(DIR_DATA, 'train', val['label'], '_'.join([val['dataset'], val['file_name']]))\n",
    "\n",
    "    try: \n",
    "        shutil.move(orig_path, dest_path)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Files\n",
    "for index, val in y_test.iterrows():\n",
    "    \n",
    "    orig_path = Path(DIR_DATA, val['dataset'], val['file_name'])\n",
    "    dest_path = Path(DIR_DATA, 'valid', val['label'], '_'.join([val['dataset'], val['file_name']]))\n",
    "\n",
    "    try: \n",
    "        shutil.move(orig_path, dest_path)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heartbeat_hack",
   "language": "python",
   "name": "heartbeat_hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
